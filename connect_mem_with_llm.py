import os

from langchain_huggingface import HuggingFaceEndpoint
from langchain_core.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

## Uncomment the following files if you're not using pipenv as your virtual environment manager
from dotenv import load_dotenv, find_dotenv
load_dotenv(find_dotenv())


# Step 1: Setup LLM (Mistral with HuggingFace)
HF_TOKEN = os.environ.get("HF_TOKEN")
HUGGINGFACE_REPO_ID = "mistralai/Mistral-7B-Instruct-v0.3"

def load_llm(huggingface_repo_id):
    llm = HuggingFaceEndpoint(
        repo_id=huggingface_repo_id,
        temperature=0.5,
        model_kwargs={"token": HF_TOKEN, "max_length": "512"}
    )
    return llm

# Step 2: Connect LLM with FAISS and Create chain

CUSTOM_PROMPT_TEMPLATE = """
        Your goal is to provide accurate, supportive, and professional responses based on the given context. 

        - Use only the information provided in the context to answer the user's question.
        - If the answer is not available in the context, kindly say, "I'm sorry, but I don't have that information."
        - Respond warmly and naturally to greetings like "hi" or "hello."
        - Maintain a compassionate, reassuring, and professional tone in all responses.
        - Keep answers concise yet informative, avoiding unnecessary details.
        - Do not mention whether context is availableâ€”just provide a clear, direct, and helpful response.
        - Focus solely on the current question without referencing previous interactions unless the user explicitly asks.

        Context: {context}  
        Question: {question}  

        Provide a thoughtful and accurate response while ensuring empathy and clarity.
        """  

def set_custom_prompt(custom_prompt_template):
    prompt = PromptTemplate(template=custom_prompt_template, input_variables=["context", "question"])
    return prompt

# Load Database
DB_FAISS_PATH = "vectorstore/db_faiss"  # Make sure this path exists and has the correct FAISS files
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
db = FAISS.load_local(DB_FAISS_PATH, embedding_model, allow_dangerous_deserialization=True)

# Create QA chain using the locally loaded LLM
qa_chain = RetrievalQA.from_chain_type(
    llm=load_llm(HUGGINGFACE_REPO_ID),  # Use the Mistral LLM loaded from HuggingFace
    chain_type="stuff",
    retriever=db.as_retriever(search_kwargs={'k': 3}),
    return_source_documents=False,
    chain_type_kwargs={'prompt': set_custom_prompt(CUSTOM_PROMPT_TEMPLATE)}
)

# Now invoke with a single query
user_query = input("Write Query Here: ")
response = qa_chain.run(user_query)  # Correct method is .run(), not .invoke()
print("RESULT: ", response)  # Directly print the response
